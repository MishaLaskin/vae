{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchsummary import summary\n",
    "\n",
    "#from pushover import notify\n",
    "#from utils import makegif\n",
    "from random import randint\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import Image, display\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Creates image dataset of 32X32 images with 3 channels\n",
    "    requires numpy and cv2 to work\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path, train=True, transform=None):\n",
    "        print('Loading data')\n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "        print('Done loading data')\n",
    "        data = np.array(data.item().get('image_observation'))\n",
    "\n",
    "        self.n = data.shape[0]\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.data[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label = 0\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Done loading data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200000, 6250)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_file_path = '/home/misha/research/vqvae/data/reacher_no_target_length100_paths_2000.npy'\n",
    "#data_file_path = '/home/misha/research/vqvae/data/two_blocks_length100_paths_100.npy'\n",
    "#data_file_path = '/home/misha/research/vqvae/data/just_place_length100_paths_600.npy'\n",
    "\n",
    "#training_data, validation_data, training_loader, validation_loader, x_train_var = utils.load_data_and_data_loaders('POINTMASS',data_file_path, 32)\n",
    "\n",
    "dataset = ImageDataset(data_file_path, train=True,\n",
    "                             transform=transforms.Compose([\n",
    "                                transforms.ToTensor(), \n",
    "                             ]))\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=bs, shuffle=True)\n",
    "len(dataset), len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fixed input for debugging\n",
    "fixed_x, _ = next(iter(dataloader))\n",
    "save_image(fixed_x, 'real_image.png')\n",
    "\n",
    "Image('real_image.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=1024):\n",
    "        return input.view(input.size(0), size, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, img_dim=32,image_channels=3, h_dim=1024, z_dim=128,device=None):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        assert img_dim == 32 or img_dim == 64, 'img_dim must be 32 or 64'\n",
    "        if img_dim == 32:\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 256, kernel_size=4, stride=2),\n",
    "                nn.ReLU(),\n",
    "                Flatten()\n",
    "            ).to(device)\n",
    "            \n",
    "            self.decoder = nn.Sequential(\n",
    "                UnFlatten(),\n",
    "                nn.ConvTranspose2d(h_dim, 128, kernel_size=5, stride=2),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(128, 32, kernel_size=6, stride=2),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "                nn.Sigmoid(),\n",
    "            ).to(device)\n",
    "        \n",
    "        if img_dim == 64:\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(128, 256, kernel_size=4, stride=2),\n",
    "                nn.ReLU(),\n",
    "                Flatten()\n",
    "            ).to(device)\n",
    "            \n",
    "            self.decoder = nn.Sequential(\n",
    "                UnFlatten(),\n",
    "                nn.ConvTranspose2d(h_dim, 128, kernel_size=5, stride=2),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "                nn.Sigmoid(),\n",
    "            ).to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(z_dim, h_dim)\n",
    "        \n",
    "        \n",
    "        self.device =device\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        # return torch.normal(mu, std)\n",
    "        esp = torch.randn(*mu.size()).to(self.device)        \n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "        \n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.fc3(z)\n",
    "        z = self.decoder(z)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x,z, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_channels = fixed_x.size(1)\n",
    "img_dim = fixed_x.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(img_dim=img_dim,image_channels=image_channels,z_dim=128,device=device).to(device)\n",
    "#model.load_state_dict(torch.load('vae.torch', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "    x = x.to(device)\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
    "    # BCE = F.mse_loss(recon_x, x, size_average=False)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD, BCE, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rfr reconstructed\n",
    "!mkdir reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/misha/miniconda3/envs/research/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/100] Loss: 1912.081 1912.046 0.035\n",
      "Epoch[2/100] Loss: 1912.138 1912.096 0.042\n",
      "Epoch[3/100] Loss: 1905.498 1905.441 0.056\n",
      "Epoch[4/100] Loss: 1897.174 1897.100 0.074\n",
      "Epoch[5/100] Loss: 1894.841 1894.768 0.073\n",
      "Epoch[6/100] Loss: 1893.860 1893.784 0.076\n",
      "Epoch[7/100] Loss: 1889.973 1889.891 0.082\n",
      "Epoch[8/100] Loss: 1887.241 1887.149 0.092\n",
      "Epoch[9/100] Loss: 1885.299 1885.206 0.093\n",
      "Epoch[10/100] Loss: 1882.647 1882.554 0.093\n",
      "Epoch[11/100] Loss: 1883.960 1883.863 0.097\n",
      "Epoch[12/100] Loss: 1880.831 1880.731 0.099\n",
      "Epoch[13/100] Loss: 1881.097 1881.004 0.093\n",
      "Epoch[14/100] Loss: 1880.639 1880.542 0.097\n",
      "Epoch[15/100] Loss: 1879.158 1879.062 0.096\n",
      "Epoch[16/100] Loss: 1878.912 1878.814 0.099\n",
      "Epoch[17/100] Loss: 1877.873 1877.777 0.096\n",
      "Epoch[18/100] Loss: 1877.894 1877.801 0.093\n",
      "Epoch[19/100] Loss: 1878.193 1878.096 0.097\n",
      "Epoch[20/100] Loss: 1878.076 1877.978 0.098\n",
      "Epoch[21/100] Loss: 1877.124 1877.029 0.095\n",
      "Epoch[22/100] Loss: 1876.779 1876.678 0.101\n",
      "Epoch[23/100] Loss: 1877.493 1877.392 0.101\n",
      "Epoch[24/100] Loss: 1877.309 1877.211 0.097\n",
      "Epoch[25/100] Loss: 1876.960 1876.862 0.098\n",
      "Epoch[26/100] Loss: 1876.739 1876.641 0.099\n",
      "Epoch[27/100] Loss: 1877.027 1876.928 0.099\n",
      "Epoch[28/100] Loss: 1877.330 1877.236 0.094\n",
      "Epoch[29/100] Loss: 1876.562 1876.468 0.094\n",
      "Epoch[30/100] Loss: 1876.401 1876.304 0.097\n",
      "Epoch[31/100] Loss: 1876.094 1875.997 0.097\n",
      "Epoch[32/100] Loss: 1875.469 1875.373 0.096\n",
      "Epoch[33/100] Loss: 1875.977 1875.882 0.095\n",
      "Epoch[34/100] Loss: 1875.855 1875.758 0.097\n",
      "Epoch[35/100] Loss: 1876.873 1876.776 0.097\n",
      "Epoch[36/100] Loss: 1875.428 1875.329 0.098\n",
      "Epoch[37/100] Loss: 1876.630 1876.532 0.098\n",
      "Epoch[38/100] Loss: 1875.683 1875.587 0.096\n",
      "Epoch[39/100] Loss: 1876.057 1875.957 0.100\n",
      "Epoch[40/100] Loss: 1875.964 1875.865 0.100\n",
      "Epoch[41/100] Loss: 1876.121 1876.025 0.096\n",
      "Epoch[42/100] Loss: 1874.719 1874.621 0.099\n",
      "Epoch[43/100] Loss: 1875.470 1875.373 0.097\n",
      "Epoch[44/100] Loss: 1875.705 1875.607 0.099\n",
      "Epoch[45/100] Loss: 1875.775 1875.678 0.097\n",
      "Epoch[46/100] Loss: 1875.849 1875.749 0.100\n",
      "Epoch[47/100] Loss: 1876.270 1876.173 0.097\n",
      "Epoch[48/100] Loss: 1876.222 1876.128 0.094\n",
      "Epoch[49/100] Loss: 1876.239 1876.140 0.099\n",
      "Epoch[50/100] Loss: 1875.438 1875.342 0.096\n",
      "Epoch[51/100] Loss: 1874.884 1874.787 0.097\n",
      "Epoch[52/100] Loss: 1875.815 1875.715 0.099\n",
      "Epoch[53/100] Loss: 1875.246 1875.148 0.098\n",
      "Epoch[54/100] Loss: 1874.902 1874.802 0.100\n",
      "Epoch[55/100] Loss: 1874.862 1874.764 0.098\n",
      "Epoch[56/100] Loss: 1875.207 1875.109 0.097\n",
      "Epoch[57/100] Loss: 1875.591 1875.494 0.097\n",
      "Epoch[58/100] Loss: 1875.061 1874.965 0.096\n",
      "Epoch[59/100] Loss: 1875.702 1875.603 0.099\n",
      "Epoch[60/100] Loss: 1875.403 1875.303 0.099\n",
      "Epoch[61/100] Loss: 1874.387 1874.286 0.100\n",
      "Epoch[62/100] Loss: 1875.341 1875.240 0.101\n",
      "Epoch[63/100] Loss: 1874.849 1874.750 0.099\n",
      "Epoch[64/100] Loss: 1874.289 1874.191 0.098\n",
      "Epoch[65/100] Loss: 1874.505 1874.405 0.099\n",
      "Epoch[66/100] Loss: 1875.834 1875.734 0.100\n",
      "Epoch[67/100] Loss: 1875.894 1875.795 0.099\n",
      "Epoch[68/100] Loss: 1875.359 1875.259 0.100\n",
      "Epoch[69/100] Loss: 1875.293 1875.194 0.099\n",
      "Epoch[70/100] Loss: 1875.135 1875.034 0.101\n",
      "Epoch[71/100] Loss: 1875.090 1874.992 0.098\n",
      "Epoch[72/100] Loss: 1874.692 1874.590 0.101\n",
      "Epoch[73/100] Loss: 1875.294 1875.195 0.100\n",
      "Epoch[74/100] Loss: 1874.671 1874.570 0.101\n",
      "Epoch[75/100] Loss: 1874.838 1874.740 0.098\n",
      "Epoch[76/100] Loss: 1874.235 1874.133 0.102\n",
      "Epoch[77/100] Loss: 1875.256 1875.154 0.103\n",
      "Epoch[78/100] Loss: 1875.115 1875.013 0.103\n",
      "Epoch[79/100] Loss: 1875.072 1874.970 0.102\n",
      "Epoch[80/100] Loss: 1875.286 1875.184 0.102\n",
      "Epoch[81/100] Loss: 1875.141 1875.040 0.101\n",
      "Epoch[82/100] Loss: 1874.724 1874.624 0.100\n",
      "Epoch[83/100] Loss: 1874.228 1874.128 0.100\n",
      "Epoch[84/100] Loss: 1874.433 1874.332 0.101\n",
      "Epoch[85/100] Loss: 1874.373 1874.271 0.101\n",
      "Epoch[86/100] Loss: 1874.847 1874.743 0.104\n",
      "Epoch[87/100] Loss: 1874.911 1874.810 0.101\n",
      "Epoch[88/100] Loss: 1874.576 1874.474 0.102\n",
      "Epoch[89/100] Loss: 1874.420 1874.318 0.102\n",
      "Epoch[90/100] Loss: 1875.587 1875.486 0.101\n",
      "Epoch[91/100] Loss: 1874.675 1874.574 0.101\n",
      "Epoch[92/100] Loss: 1874.698 1874.594 0.104\n",
      "Epoch[93/100] Loss: 1874.496 1874.395 0.101\n",
      "Epoch[94/100] Loss: 1874.957 1874.854 0.103\n",
      "Epoch[95/100] Loss: 1875.093 1874.993 0.100\n",
      "Epoch[96/100] Loss: 1874.850 1874.749 0.101\n",
      "Epoch[97/100] Loss: 1874.022 1873.919 0.103\n",
      "Epoch[98/100] Loss: 1874.466 1874.363 0.103\n",
      "Epoch[99/100] Loss: 1875.182 1875.079 0.103\n",
      "Epoch[100/100] Loss: 1874.785 1874.681 0.104\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'saved_models/one_block_vae.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-fcad3f65c0d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#notify(to_print, priority=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'saved_models/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \"\"\"\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'saved_models/one_block_vae.pth'"
     ]
    }
   ],
   "source": [
    "filename = 'one_block_vae.pth'\n",
    "epoch = 0\n",
    "while epoch < epochs:\n",
    "    \n",
    "    for idx, (images, _) in enumerate(dataloader):\n",
    "        recon_images,z, mu, logvar = model(images.to(device))\n",
    "        loss, bce, kld = loss_fn(recon_images, images, mu, logvar)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        to_print = \"Epoch[{}/{}] Loss: {:.3f} {:.3f} {:.3f}\".format(epoch+1, \n",
    "                                epochs, loss.item()/bs, bce.item()/bs, kld.item()/bs)\n",
    "        if idx % 100 == 0 and idx >0:\n",
    "            epoch+=1\n",
    "            print(to_print)\n",
    "            break\n",
    "\n",
    "# notify to android when finished training\n",
    "#notify(to_print, priority=1)\n",
    "\n",
    "torch.save(model.state_dict(), 'saved_models/' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = 'reacher_vae.pth'\n",
    "#torch.save(model.state_dict(), 'saved_models/' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(x):\n",
    "    x=x.to(device)\n",
    "    recon_x,_, _, _ = model(x)\n",
    "    return torch.cat([x, recon_x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/misha/research/baselines/vae/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VAE:\n\tUnexpected key(s) in state_dict: \"encoder.6.weight\", \"encoder.6.bias\", \"decoder.7.weight\", \"decoder.7.bias\". \n\tsize mismatch for encoder.4.weight: copying a param with shape torch.Size([128, 64, 4, 4]) from checkpoint, the shape in current model is torch.Size([256, 64, 4, 4]).\n\tsize mismatch for encoder.4.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.3.weight: copying a param with shape torch.Size([128, 64, 5, 5]) from checkpoint, the shape in current model is torch.Size([128, 32, 6, 6]).\n\tsize mismatch for decoder.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for decoder.5.weight: copying a param with shape torch.Size([64, 32, 6, 6]) from checkpoint, the shape in current model is torch.Size([32, 3, 6, 6]).\n\tsize mismatch for decoder.5.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([3]).\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([32, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n\tsize mismatch for fc1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([32, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n\tsize mismatch for fc2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for fc3.weight: copying a param with shape torch.Size([1024, 32]) from checkpoint, the shape in current model is torch.Size([1024, 128]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-83f203b61837>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/misha/research/baselines/vae/saved_models/one_block_vae.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreacher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReacher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 845\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VAE:\n\tUnexpected key(s) in state_dict: \"encoder.6.weight\", \"encoder.6.bias\", \"decoder.7.weight\", \"decoder.7.bias\". \n\tsize mismatch for encoder.4.weight: copying a param with shape torch.Size([128, 64, 4, 4]) from checkpoint, the shape in current model is torch.Size([256, 64, 4, 4]).\n\tsize mismatch for encoder.4.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.3.weight: copying a param with shape torch.Size([128, 64, 5, 5]) from checkpoint, the shape in current model is torch.Size([128, 32, 6, 6]).\n\tsize mismatch for decoder.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for decoder.5.weight: copying a param with shape torch.Size([64, 32, 6, 6]) from checkpoint, the shape in current model is torch.Size([32, 3, 6, 6]).\n\tsize mismatch for decoder.5.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([3]).\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([32, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n\tsize mismatch for fc1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([32, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n\tsize mismatch for fc2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for fc3.weight: copying a param with shape torch.Size([1024, 32]) from checkpoint, the shape in current model is torch.Size([1024, 128])."
     ]
    }
   ],
   "source": [
    "path = '/home/misha/research/baselines/vae/saved_models/one_block_vae.pth'\n",
    "\n",
    "model.load_state_dict(torch.load(path))\n",
    "from vae.envs.reacher import Reacher\n",
    "\n",
    "reacher = Reacher()\n",
    "model = reacher.model\n",
    "\n",
    "# sample = torch.randn(bs, 1024)\n",
    "# compare_x = vae.decoder(sample)\n",
    "\n",
    "# fixed_x, _ = next(iter(dataloader))\n",
    "# fixed_x = fixed_x[:8]\n",
    "fixed_x = dataset[randint(1, 100)][0].unsqueeze(0)\n",
    "compare_x = compare(fixed_x)\n",
    "\n",
    "save_image(compare_x.data.cpu(), 'sample_image.png')\n",
    "display(Image('sample_image.png', width=700, unconfined=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_x.shape\n",
    "_,z,_,_ = model(fixed_x.to(device))\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
